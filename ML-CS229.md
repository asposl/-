# ML-CS229

## 线性回归

1. 梯度下降
2. 随机梯度下降
3. 自适应学习率
4. [正规方程](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC2%E6%AD%A5%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/05.1-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95.html)（只适用于线性回归）

## 局部加权回归

## Logistics回归

- 牛顿法

## [Perceptron](https://zh.m.wikipedia.org/zh/%E6%84%9F%E7%9F%A5%E5%99%A8)

## GLM(Generalized linear model)

$P(y;\eta) = b(y)\cdot \exp \left[\eta^T \cdot T(y) - a(\eta)\right]$ 

## GDA(Gaussian Discriminant Analysis)

Caronical Response Function

Caronical Link Function

% 如果协方差不相同怎么办

## Naive Bayes

Laplace smooth

% 在文本分析中，字频和字出现的顺序都有意义

## SVM

不如Neural Network，但超参数少。

天下没有免费的午餐定理

归纳偏差

### Kernels trick

### SMO算法

坐标上升法
